services:
  inference_api:
    image: openvino/model_server:latest
    command: |
      --model_name challenge2
      --model_path /models
      --layout NCHW:NCHW
      --rest_port 9001
      --port 9000
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - ./models:/models:ro
  
  app_web:
    build: app_web
    depends_on:
      - inference_api
    environment:
      - PORT=8002
      - API_HOST=http://inference_api:9001
      - MODEL_NAME=challenge2
    ports:
      - 8002:8002
